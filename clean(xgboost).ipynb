{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d70d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uuid', 'ord_in_thread', 'author', 'published', 'title', 'text',\n",
      "       'language', 'crawled', 'site_url', 'country', 'domain_rank',\n",
      "       'thread_title', 'spam_score', 'main_img_url', 'replies_count',\n",
      "       'participants_count', 'likes', 'comments', 'shares', 'type'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yanis\\AppData\\Local\\Temp\\ipykernel_13424\\1406899808.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['author'].fillna('unknown', inplace=True)\n",
      "C:\\Users\\Yanis\\AppData\\Local\\Temp\\ipykernel_13424\\1406899808.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"domain_rank\"].fillna(1_000_000, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# ===============================\n",
    "# 1. Load dataset\n",
    "# ===============================\n",
    "\n",
    "df = pd.read_csv(\"fake.csv\")\n",
    "\n",
    "# ===============================\n",
    "# 2. Preprocessing \n",
    "# ===============================\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "drop_idx = df[df[\"type\"] == \"bs\"].sample(n=11000, random_state=42).index\n",
    "\n",
    "# Drop them from the dataframe\n",
    "df = df.drop(drop_idx)\n",
    "\n",
    "for i in ['uuid', 'ord_in_thread', 'crawled', 'site_url','spam_score', 'main_img_url']:\n",
    "    del df[i]\n",
    "    \n",
    "df['author'].fillna('unknown', inplace=True)\n",
    "\n",
    "df[\"domain_rank\"].fillna(1_000_000, inplace=True)\n",
    "\n",
    "df.dropna(subset=['text','title','domain_rank'], inplace=True)\n",
    "\n",
    "\n",
    "languages = [\n",
    "    'english', 'ignore', 'german', 'french', 'spanish', 'russian', 'greek',\n",
    "    'finnish', 'portuguese', 'arabic', 'dutch', 'italian', 'turkish',\n",
    "    'norwegian', 'chinese', 'polish'\n",
    "]\n",
    "\n",
    "countries = [\n",
    "    'US', 'N/A', 'DE', 'FR', 'ES', 'RU', 'GR',\n",
    "    'FI', 'PT', 'DZ', 'NL', 'IT',\n",
    "    'TR', 'NO', 'CN', 'PL'\n",
    "]\n",
    "\n",
    "language_to_country = dict(zip(languages, countries))\n",
    "\n",
    "df[\"country\"] = df[\"language\"].map(language_to_country)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f88debbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "#turning ['bias' 'conspiracy' 'fake' 'bs' 'satire' 'hate' 'junksci' 'state'] into [0,1,2,3,4,5,6,7]\n",
    "type_mapping = {t: i for i, t in enumerate(df[\"type\"].unique())}\n",
    "df[\"type\"] = df[\"type\"].map(type_mapping)\n",
    "\n",
    "print(df[\"type\"].unique())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e469f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yanis\\AppData\\Local\\Temp\\ipykernel_13424\\189953854.py:14: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"published\"].iloc[i] = str(timestamp)\n"
     ]
    }
   ],
   "source": [
    "#preprocsessing published\n",
    "#converting the object into strings\n",
    "print(df['published'].dtype)\n",
    "df['published'] = df['published'].astype('string')\n",
    "print(df['published'].dtype)\n",
    "\n",
    "#using \n",
    "import datetime\n",
    "\n",
    "for i in range(len(df[\"published\"])):\n",
    "    idk  =df[\"published\"].iloc[i]\n",
    "    time  = datetime.datetime.fromisoformat(idk)\n",
    "    timestamp = time.timestamp()\n",
    "    df[\"published\"].iloc[i] = str(timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38914c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "#converting the string of time stamps into floats \n",
    "print(df['published'].dtype)\n",
    "df['published'] = df['published'].astype('float')\n",
    "print(df['published'].dtype)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7056d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1818, 10000)\n"
     ]
    }
   ],
   "source": [
    "#tf idf to work on the string columns \n",
    "\n",
    "df[\"combined\"] = (\n",
    "    df[\"author\"].astype(str) + \" \" +\n",
    "    df[\"published\"].astype(str) + \" \" +\n",
    "    df[\"title\"].astype(str) + \" \" +\n",
    "    df[\"text\"].astype(str) + \" \" +\n",
    "    df[\"language\"].astype(str) + \" \" +\n",
    "    df[\"country\"].astype(str) + \" \" +\n",
    "    df[\"thread_title\"].astype(str)\n",
    ")\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    stop_words=\"english\",       # remove common words like \"the\", \"in\", \"and\"\n",
    "    ngram_range=(1,2),          # optional: use unigrams + bigrams for richer features\n",
    "    lowercase=True              # ensures everything is lowercase\n",
    ") \n",
    "tfidf_matrix = tfidf.fit_transform(df[\"combined\"])\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f086f326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author : string\n",
      "published : float64\n",
      "title : string\n",
      "text : string\n",
      "language : string\n",
      "country : string\n",
      "domain_rank : float64\n",
      "thread_title : string\n",
      "replies_count : int64\n",
      "participants_count : int64\n",
      "likes : int64\n",
      "comments : int64\n",
      "shares : int64\n",
      "type : int64\n",
      "combined : string\n"
     ]
    }
   ],
   "source": [
    "#to see the type we are dealing with\n",
    "\n",
    "#converting every \"object\" column into a string\n",
    "\n",
    "\n",
    "for i in [\"author\",\"title\",\"text\",\"language\",\"country\",\"thread_title\",\"combined\"]:\n",
    "    df[i] = df[i].astype('string') \n",
    "\n",
    "for i in df.columns: \n",
    "   print(i,\":\",df[i].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c9322904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: {3: np.int64(993), 1: np.int64(288), 0: np.int64(248), 5: np.int64(172), 7: np.int64(85), 6: np.int64(72), 4: np.int64(70), 2: np.int64(13)}\n",
      "After SMOTE: {1: np.int64(993), 3: np.int64(993), 0: np.int64(993), 6: np.int64(993), 7: np.int64(993), 4: np.int64(993), 5: np.int64(993), 2: np.int64(993)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:34:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     34\u001b[39m model = XGBClassifier(\n\u001b[32m     35\u001b[39m     n_estimators=\u001b[32m500\u001b[39m,\n\u001b[32m     36\u001b[39m     max_depth=\u001b[32m4\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     eval_metric=\u001b[33m\"\u001b[39m\u001b[33mmlogloss\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m )\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Train on resampled data (no sample_weight here, SMOTE handled imbalance)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m     53\u001b[39m y_pred = model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1683\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1661\u001b[39m model, metric, params, feature_weights = \u001b[38;5;28mself\u001b[39m._configure_fit(\n\u001b[32m   1662\u001b[39m     xgb_model, params, feature_weights\n\u001b[32m   1663\u001b[39m )\n\u001b[32m   1664\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1665\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1666\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1680\u001b[39m     feature_types=\u001b[38;5;28mself\u001b[39m.feature_types,\n\u001b[32m   1681\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1683\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1698\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Prepare numeric features\n",
    "numeric_features = df[[\n",
    "    \"domain_rank\", \"likes\", \"comments\", \"shares\",\n",
    "    \"participants_count\", \"replies_count\", \"published\"\n",
    "]].values\n",
    "\n",
    "# Ensure numeric_features is float for hstack\n",
    "numeric_features = numeric_features.astype(float)\n",
    "\n",
    "# Combine text + numeric\n",
    "X = hstack([tfidf_matrix, numeric_features])\n",
    "y = df[\"type\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE only on the training set\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", dict(pd.Series(y_train).value_counts()))\n",
    "print(\"After SMOTE:\", dict(pd.Series(y_train_resampled).value_counts()))\n",
    "\n",
    "# Define XGBoost model (regularized to reduce overfitting)\n",
    "model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    min_child_weight=5,\n",
    "    gamma=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"mlogloss\"\n",
    ")\n",
    "\n",
    "# Train on resampled data (no sample_weight here, SMOTE handled imbalance)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b15fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "dfs = []\n",
    "for label, group in df.groupby(\"type\"):\n",
    "    if len(group) < 400:\n",
    "        group_upsampled = resample(\n",
    "            group,\n",
    "            replace=True,\n",
    "            n_samples=400,\n",
    "            random_state=42\n",
    "        )\n",
    "        dfs.append(group_upsampled)\n",
    "    else:\n",
    "        dfs.append(group)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07036cae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 3276, expected 1818.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     10\u001b[39m numeric_features = df[[\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdomain_rank\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlikes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcomments\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mshares\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mparticipants_count\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mreplies_count\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpublished\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m ]].values.astype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Combine text + numeric\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X = \u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtfidf_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m y = df[\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Ensure CSR format\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\sparse\\_construct.py:801\u001b[39m, in \u001b[36mhstack\u001b[39m\u001b[34m(blocks, format, dtype)\u001b[39m\n\u001b[32m    799\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _block([blocks], \u001b[38;5;28mformat\u001b[39m, dtype)\n\u001b[32m    800\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_spmatrix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\sparse\\_construct.py:1016\u001b[39m, in \u001b[36m_block\u001b[39m\u001b[34m(blocks, format, dtype, return_spmatrix)\u001b[39m\n\u001b[32m   1012\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m brow_lengths[i] != A._shape_as_2d[\u001b[32m0\u001b[39m]:\n\u001b[32m   1013\u001b[39m     msg = (\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mblocks[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,:] has incompatible row dimensions. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1014\u001b[39m            \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mGot blocks[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m].shape[0] == \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA._shape_as_2d[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1015\u001b[39m            \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrow_lengths[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bcol_lengths[j] == \u001b[32m0\u001b[39m:\n\u001b[32m   1019\u001b[39m     bcol_lengths[j] = A._shape_as_2d[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 3276, expected 1818."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import coo_matrix\n",
    "import numpy as np\n",
    "\n",
    "##################################RANDOM FOREST################################################\n",
    "\n",
    "# Prepare numeric features                    \n",
    "numeric_features = df[[\n",
    "    \"domain_rank\", \"likes\", \"comments\", \"shares\",\n",
    "    \"participants_count\", \"replies_count\", \"published\"\n",
    "]].values.astype(float)\n",
    "\n",
    "# Combine text + numeric\n",
    "X = hstack([tfidf_matrix, numeric_features])\n",
    "y = df[\"type\"]\n",
    "\n",
    "# Ensure CSR format\n",
    "if isinstance(X, coo_matrix):\n",
    "    X = X.tocsr()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Random Forest model renamed to model1\n",
    "    model1 = RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        max_depth=10,\n",
    "        min_samples_split=15,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model1.fit(X_train, y_train)\n",
    "\n",
    "    # Training accuracy\n",
    "    y_train_pred = model1.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "    # Validation accuracy\n",
    "    y_pred = model1.predict(X_test)\n",
    "    valid_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Fold {fold}: train acc = {train_acc:.4f}, valid acc = {valid_acc:.4f}\")\n",
    "\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "\n",
    "# Print means\n",
    "print(\"\\nMean train accuracy:\", np.mean(train_accuracies))\n",
    "print(\"Mean valid accuracy:\", np.mean(valid_accuracies))\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save vectorizer + model\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(model1, \"xgb_model.pkl\")\n",
    "\n",
    "import joblib\n",
    "import datetime\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load saved vectorizer and model\n",
    "tfidf_vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "xgb_model = joblib.load(\"xgb_model.pkl\")\n",
    "\n",
    "#example1 : # Predicted type: \"conspiracy\"\n",
    "# New record\n",
    "new_record = {\n",
    "    \"author\": \"Jane Smith\",\n",
    "    \"published\": \"2023-08-15T12:30:00\",\n",
    "    \"title\": \"Hidden Truth Behind Moon Landing\",\n",
    "    \"text\": \"Many believe the Apollo missions were staged in Hollywood studios. Evidence suggests photos contain lighting anomalies that could not exist on the Moon.\",\n",
    "    \"language\": \"en\",\n",
    "    \"country\": \"US\",\n",
    "    \"domain_rank\": 5321,\n",
    "    \"thread_title\": \"Moon landing conspiracy resurfaces\",\n",
    "    \"replies_count\": 56,\n",
    "    \"participants_count\": 24,\n",
    "    \"likes\": 180,\n",
    "    \"comments\": 90,\n",
    "    \"shares\": 45\n",
    "}\n",
    "\n",
    "# Convert published into timestamp\n",
    "time_new = datetime.datetime.fromisoformat(new_record[\"published\"])\n",
    "timestamp_new = time_new.timestamp()\n",
    "\n",
    "# Build combined text (same way as training)\n",
    "combined_text = (\n",
    "    str(new_record[\"author\"]) + \" \" +\n",
    "    str(timestamp_new) + \" \" +\n",
    "    str(new_record[\"title\"]) + \" \" +\n",
    "    str(new_record[\"text\"]) + \" \" +\n",
    "    str(new_record[\"language\"]) + \" \" +\n",
    "    str(new_record[\"country\"]) + \" \" +\n",
    "    str(new_record[\"thread_title\"])\n",
    ")\n",
    "\n",
    "# --- Transform with the trained vectorizer ---\n",
    "new_tfidf = tfidf_vectorizer.transform([combined_text])\n",
    "\n",
    "# --- Numeric features (same order as training) ---\n",
    "new_numeric = np.array([[\n",
    "    new_record[\"domain_rank\"],\n",
    "    new_record[\"likes\"],\n",
    "    new_record[\"comments\"],\n",
    "    new_record[\"shares\"],\n",
    "    new_record[\"participants_count\"],\n",
    "    new_record[\"replies_count\"],\n",
    "    timestamp_new\n",
    "]], dtype=float)\n",
    "\n",
    "# --- Combine TF-IDF + numeric ---\n",
    "new_X = hstack([new_tfidf, new_numeric])\n",
    "\n",
    "# --- Predict ---\n",
    "\n",
    "list = ['bias', 'conspiracy', 'fake' ,'bs' ,'satire' ,'hate' ,'junksci' ,'state']\n",
    "\n",
    "prediction = xgb_model.predict(new_X)\n",
    "print(\"Predicted type:\", list[prediction[0]])\n",
    "\n",
    "#example2 : hate\n",
    "\n",
    "new1_record = {\n",
    "    \"author\": \"Unknown\",\n",
    "    \"published\": \"2021-07-22T20:00:00+02:00\",\n",
    "    \"title\": \"Group X is destroying our country\",\n",
    "    \"text\": \"People from Group X are ruining our traditions and must be stopped.\",\n",
    "    \"language\": \"english\",\n",
    "    \"country\": \"US\",\n",
    "    \"domain_rank\": 600000,\n",
    "    \"thread_title\": \"Group X Hate\",\n",
    "    \"replies_count\": 500,\n",
    "    \"participants_count\": 200,\n",
    "    \"likes\": 1000,\n",
    "    \"comments\": 800,\n",
    "    \"shares\": 1200\n",
    "}\n",
    "\n",
    "# Convert published into timestamp\n",
    "time_new1 = datetime.datetime.fromisoformat(new1_record[\"published\"])\n",
    "timestamp_new1 = time_new1.timestamp()\n",
    "\n",
    "# Build combined text (same way as training)\n",
    "combined_text1 = (\n",
    "    str(new1_record[\"author\"]) + \" \" +\n",
    "    str(timestamp_new1) + \" \" +\n",
    "    str(new1_record[\"title\"]) + \" \" +\n",
    "    str(new1_record[\"text\"]) + \" \" +\n",
    "    str(new1_record[\"language\"]) + \" \" +\n",
    "    str(new1_record[\"country\"]) + \" \" +\n",
    "    str(new1_record[\"thread_title\"])\n",
    ")\n",
    "\n",
    "# --- Transform with the trained vectorizer ---\n",
    "new1_tfidf = tfidf_vectorizer.transform([combined_text1])\n",
    "\n",
    "# --- Numeric features (same order as training) ---\n",
    "new1_numeric = np.array([[\n",
    "    new1_record[\"domain_rank\"],\n",
    "    new1_record[\"likes\"],\n",
    "    new1_record[\"comments\"],\n",
    "    new1_record[\"shares\"],\n",
    "    new1_record[\"participants_count\"],\n",
    "    new1_record[\"replies_count\"],\n",
    "    timestamp_new1\n",
    "]], dtype=float)\n",
    "\n",
    "# --- Combine TF-IDF + numeric ---\n",
    "new1_X = hstack([new1_tfidf, new1_numeric])\n",
    "\n",
    "# --- Predict ---\n",
    "prediction1 = xgb_model.predict(new1_X)\n",
    "\n",
    "list = ['bias', 'conspiracy', 'fake' ,'bs' ,'satire' ,'hate' ,'junksci' ,'state']\n",
    "\n",
    "print(\"Predicted type:\", list[prediction1[0]])\n",
    "\n",
    "\n",
    "#example3 : satire\n",
    "\n",
    "new2_record = {\n",
    "    \"author\": \"Satire News\",\n",
    "    \"published\": \"2023-01-01T08:00:00+02:00\",\n",
    "    \"title\": \"Aliens demand free Wi-Fi from world leaders\",\n",
    "    \"text\": \"In a shocking event, aliens have refused to leave Earth until they get unlimited internet access.\",\n",
    "    \"language\": \"english\",\n",
    "    \"country\": \"CA\",\n",
    "    \"domain_rank\": 20000,\n",
    "    \"thread_title\": \"Aliens protest\",\n",
    "    \"replies_count\": 80,\n",
    "    \"participants_count\": 40,\n",
    "    \"likes\": 5000,\n",
    "    \"comments\": 1200,\n",
    "    \"shares\": 2000\n",
    "}\n",
    "\n",
    "# Convert published into timestamp\n",
    "time_new2 = datetime.datetime.fromisoformat(new2_record[\"published\"])\n",
    "timestamp_new2 = time_new2.timestamp()\n",
    "\n",
    "# Build combined text (same way as training)\n",
    "combined_text1 = (\n",
    "    str(new2_record[\"author\"]) + \" \" +\n",
    "    str(timestamp_new2) + \" \" +\n",
    "    str(new2_record[\"title\"]) + \" \" +\n",
    "    str(new2_record[\"text\"]) + \" \" +\n",
    "    str(new2_record[\"language\"]) + \" \" +\n",
    "    str(new2_record[\"country\"]) + \" \" +\n",
    "    str(new2_record[\"thread_title\"])\n",
    ")\n",
    "\n",
    "# --- Transform with the trained vectorizer ---\n",
    "new2_tfidf = tfidf_vectorizer.transform([combined_text1])\n",
    "\n",
    "# --- Numeric features (same order as training) ---\n",
    "new2_numeric = np.array([[\n",
    "    new2_record[\"domain_rank\"],\n",
    "    new2_record[\"likes\"],\n",
    "    new2_record[\"comments\"],\n",
    "    new2_record[\"shares\"],\n",
    "    new2_record[\"participants_count\"],\n",
    "    new2_record[\"replies_count\"],\n",
    "    timestamp_new2\n",
    "]], dtype=float)\n",
    "\n",
    "# --- Combine TF-IDF + numeric ---\n",
    "new2_X = hstack([new2_tfidf, new2_numeric])\n",
    "\n",
    "# --- Predict ---\n",
    "prediction1 = xgb_model.predict(new2_X)\n",
    "\n",
    "list = ['bias', 'conspiracy', 'fake' ,'bs' ,'satire' ,'hate' ,'junksci' ,'state']\n",
    "\n",
    "print(\"Predicted type:\", list[prediction1[0]])\n",
    "\n",
    "#example4 : bias\n",
    "\n",
    "new3_record = {\n",
    "    \"author\": \"Political Blogger\",\n",
    "    \"published\": \"2022-05-10T09:00:00+02:00\",\n",
    "    \"title\": \"Only Party Z can save the country\",\n",
    "    \"text\": \"Party Z is the only choice for true patriots, the opposition wants to destroy everything we stand for.\",\n",
    "    \"language\": \"english\",\n",
    "    \"country\": \"US\",\n",
    "    \"domain_rank\": 300000,\n",
    "    \"thread_title\": \"Elections 2022\",\n",
    "    \"replies_count\": 400,\n",
    "    \"participants_count\": 150,\n",
    "    \"likes\": 2000,\n",
    "    \"comments\": 900,\n",
    "    \"shares\": 500\n",
    "}\n",
    "\n",
    "# Convert published into timestamp\n",
    "time_new3 = datetime.datetime.fromisoformat(new_record[\"published\"])\n",
    "timestamp_new3 = time_new3.timestamp()\n",
    "\n",
    "# Build combined text (same way as training)\n",
    "combined_text1 = (\n",
    "    str(new3_record[\"author\"]) + \" \" +\n",
    "    str(timestamp_new3) + \" \" +\n",
    "    str(new3_record[\"title\"]) + \" \" +\n",
    "    str(new3_record[\"text\"]) + \" \" +\n",
    "    str(new3_record[\"language\"]) + \" \" +\n",
    "    str(new3_record[\"country\"]) + \" \" +\n",
    "    str(new3_record[\"thread_title\"])\n",
    ")\n",
    "\n",
    "# --- Transform with the trained vectorizer ---\n",
    "new3_tfidf = tfidf_vectorizer.transform([combined_text1])\n",
    "\n",
    "# --- Numeric features (same order as training) ---\n",
    "new3_numeric = np.array([[\n",
    "    new3_record[\"domain_rank\"],\n",
    "    new3_record[\"likes\"],\n",
    "    new3_record[\"comments\"],\n",
    "    new3_record[\"shares\"],\n",
    "    new3_record[\"participants_count\"],\n",
    "    new3_record[\"replies_count\"],\n",
    "    timestamp_new3\n",
    "]], dtype=float)\n",
    "\n",
    "# --- Combine TF-IDF + numeric ---\n",
    "new3_X = hstack([new3_tfidf, new3_numeric])\n",
    "\n",
    "# --- Predict ---\n",
    "prediction1 = xgb_model.predict(new3_X)\n",
    "\n",
    "list = ['bias', 'conspiracy', 'fake' ,'bs' ,'satire' ,'hate' ,'junksci' ,'state']\n",
    "\n",
    "print(\"Predicted type:\", list[prediction1[0]])\n",
    "\n",
    "\n",
    "#example5 : junksci\n",
    "\n",
    "new4_record = {\n",
    "    \"author\": \"Health Guru\",\n",
    "    \"published\": \"2021-09-20T14:00:00+02:00\",\n",
    "    \"title\": \"Drink lemon water to cure all diseases\",\n",
    "    \"text\": \"Doctors don’t want you to know this secret: lemon water cures cancer, diabetes, and heart disease.\",\n",
    "    \"language\": \"english\",\n",
    "    \"country\": \"AU\",\n",
    "    \"domain_rank\": 600000,\n",
    "    \"thread_title\": \"Natural cures\",\n",
    "    \"replies_count\": 220,\n",
    "    \"participants_count\": 90,\n",
    "    \"likes\": 800,\n",
    "    \"comments\": 400,\n",
    "    \"shares\": 350\n",
    "}\n",
    "\n",
    "# Convert published into timestamp\n",
    "time_new4 = datetime.datetime.fromisoformat(new_record[\"published\"])\n",
    "timestamp_new4 = time_new4.timestamp()\n",
    "\n",
    "# Build combined text (same way as training)\n",
    "combined_text1 = (\n",
    "    str(new4_record[\"author\"]) + \" \" +\n",
    "    str(timestamp_new4) + \" \" +\n",
    "    str(new4_record[\"title\"]) + \" \" +\n",
    "    str(new4_record[\"text\"]) + \" \" +\n",
    "    str(new4_record[\"language\"]) + \" \" +\n",
    "    str(new4_record[\"country\"]) + \" \" +\n",
    "    str(new4_record[\"thread_title\"])\n",
    ")\n",
    "\n",
    "# --- Transform with the trained vectorizer ---\n",
    "new4_tfidf = tfidf_vectorizer.transform([combined_text1])\n",
    "\n",
    "# --- Numeric features (same order as training) ---\n",
    "new4_numeric = np.array([[\n",
    "    new4_record[\"domain_rank\"],\n",
    "    new4_record[\"likes\"],\n",
    "    new4_record[\"comments\"],\n",
    "    new4_record[\"shares\"],\n",
    "    new4_record[\"participants_count\"],\n",
    "    new4_record[\"replies_count\"],\n",
    "    timestamp_new4\n",
    "]], dtype=float)\n",
    "\n",
    "# --- Combine TF-IDF + numeric ---\n",
    "new4_X = hstack([new4_tfidf, new4_numeric])\n",
    "\n",
    "# --- Predict ---\n",
    "prediction1 = xgb_model.predict(new4_X)\n",
    "\n",
    "list = ['bias', 'conspiracy', 'fake' ,'bs' ,'satire' ,'hate' ,'junksci' ,'state']\n",
    "\n",
    "print(\"Predicted type:\", list[prediction1[0]])\n",
    "\n",
    "\n",
    "#bs\n",
    "new_record_bs = {\n",
    "    \"author\": \"Anonymous\",\n",
    "    \"published\": \"2021-09-15T14:30:00+02:00\",\n",
    "    \"title\": \"10 Shocking Secrets About Water You Didn’t Know\",\n",
    "    \"text\": \"Scientists are hiding the truth! Drinking tap water turns your brain into mush. Big corporations don’t want you to know the secret of pure water crystals.\",\n",
    "    \"language\": \"english\",\n",
    "    \"country\": \"US\",\n",
    "    \"domain_rank\": 500000,\n",
    "    \"thread_title\": \"Water Conspiracy?\",\n",
    "    \"replies_count\": 350,\n",
    "    \"participants_count\": 120,\n",
    "    \"likes\": 800,\n",
    "    \"comments\": 600,\n",
    "    \"shares\": 1000\n",
    "}\n",
    "\n",
    "# Convert published into timestamp\n",
    "time_newbs = datetime.datetime.fromisoformat(new_record[\"published\"])\n",
    "timestamp_newbs = time_newbs.timestamp()\n",
    "\n",
    "# Build combined text (same way as training)\n",
    "combined_textbs = (\n",
    "    str(new_record_bs[\"author\"]) + \" \" +\n",
    "    str(timestamp_newbs) + \" \" +\n",
    "    str(new_record_bs[\"title\"]) + \" \" +\n",
    "    str(new_record_bs[\"text\"]) + \" \" +\n",
    "    str(new_record_bs[\"language\"]) + \" \" +\n",
    "    str(new_record_bs[\"country\"]) + \" \" +\n",
    "    str(new_record_bs[\"thread_title\"])\n",
    ")\n",
    "\n",
    "# --- Transform with the trained vectorizer ---\n",
    "newbs_tfidf = tfidf_vectorizer.transform([combined_textbs])\n",
    "\n",
    "# --- Numeric features (same order as training) ---\n",
    "newbs_numeric = np.array([[\n",
    "    new_record_bs[\"domain_rank\"],\n",
    "    new_record_bs[\"likes\"],\n",
    "    new_record_bs[\"comments\"],\n",
    "    new_record_bs[\"shares\"],\n",
    "    new_record_bs[\"participants_count\"],\n",
    "    new_record_bs[\"replies_count\"],\n",
    "    timestamp_new4\n",
    "]], dtype=float)\n",
    "\n",
    "# --- Combine TF-IDF + numeric ---\n",
    "newbs_X = hstack([newbs_tfidf, newbs_numeric])\n",
    "\n",
    "# --- Predict ---\n",
    "prediction1 = xgb_model.predict(newbs_X)\n",
    "\n",
    "list = ['bias', 'conspiracy', 'fake' ,'bs' ,'satire' ,'hate' ,'junksci' ,'state']\n",
    "\n",
    "print(\"Predicted type:\", list[prediction1[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe70d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "3    464\n",
      "1    412\n",
      "0    354\n",
      "5    246\n",
      "7    121\n",
      "6    102\n",
      "4    100\n",
      "2     19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"type\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa503ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "3    464\n",
      "1    412\n",
      "6    400\n",
      "7    400\n",
      "0    400\n",
      "4    400\n",
      "2    400\n",
      "5    400\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df[\"type\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
